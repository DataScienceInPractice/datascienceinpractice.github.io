{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Data Gathering is simply the process of collecting your data together.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers strategies you can use to gather data for an analysis. \n",
    "\n",
    "If you want to move on to first working on data analyses (with provided data) you can move onto the next tutorials, and come back to this one later.\n",
    "\n",
    "Data gathering can encompass anything from launching a data collection project, web scraping, pulling from a database, downloading data in bulk. \n",
    "\n",
    "It might even include simply calling someone to ask if you can use some of their data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to get Data\n",
    "\n",
    "### The Web \n",
    "\n",
    "The web is absolutely full of data or ways to get data, either by hosting **data repositories** from which you can download data, by offering **APIs** through which you can request specific data from particular applications, or as data itself, such that you can use **web scraping** to extract data directly from websites. \n",
    "\n",
    "### Other than the Web\n",
    "\n",
    "Not all data is indexed or accessible on the web, at least not publicly. \n",
    "\n",
    "Sometimes finding data means chasing down data wherever it might be. \n",
    "\n",
    "If there is some particular data you need, you can try to figure out who might have it, and get in touch to see if it might be available.\n",
    "\n",
    "### Data Gathering Skills\n",
    "Depending on your gathering method, you will likely have to do some combination of the following:\n",
    "- Download data files from repositories\n",
    "- Read data files into python\n",
    "- Use APIs \n",
    "- Query databases\n",
    "- Call someone and ask them to send you a harddrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "A Data Repository is basically just a place that data is stored. For our purposes, it is a place you can download data from. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "There is a curated list of good data source included in the \n",
    "<a href=\"https://github.com/COGS108/Projects\" class=\"alert-link\">project materials</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "A database is an organized collection of data. More formally, 'database' refers to a set of related data, and the way it is organized. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Query Language - SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "SQL (pronounced 'sequel') is a language used to 'communicate' with databases, making queries to request particular data from them.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "There is a useful introduction and tutorial to SQL\n",
    "<a href=\"http://www.sqlcourse.com/intro.html\" class=\"alert-link\">here</a>\n",
    "as well as some useful 'cheat sheets' \n",
    "<a href=\"http://www.cheat-sheets.org/sites/sql.su/\" class=\"alert-link\">here</a>\n",
    "and\n",
    "<a href=\"http://www.sqltutorial.org/wp-content/uploads/2016/04/SQL-cheat-sheet.pdf\" class=\"alert-link\">here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL is the standard, and most popular, way to interface with relational databases.\n",
    "\n",
    "Note: None of the rest of the tutorials presume or require any knowledge of SQL. \n",
    "\n",
    "You can look into it if you want, or if it is relevant to accessing some data you want to analyze, but it is not required for this set of tutorials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Program Interfaces (APIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "APIs are basically a way for software to talk to software - it is an interface into an application / website / database designed for software.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "For a simple explanation of APIs go\n",
    "<a href=\"https://medium.freecodecamp.com/what-is-an-api-in-english-please-b880a3214a82\" class=\"alert-link\">here</a>\n",
    "or for a much broader, more technical, overview try\n",
    "<a href=\"https://medium.com/@mattburgess/apis-a-basic-primer-f8250602597d\" class=\"alert-link\">here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APIs offer a lot of functionality - you can send requests to the application to do all kinds of actions. In fact, any application interface that is designed to be used programatically is an API, including, for example, interfaces for using packages of code. \n",
    "\n",
    "One of the many things that APIs do, and offer, is a way to query and access data from particular applications / databases. The benefit of using APIs for data gathering purposes is that they typically return data in nicely structured formats, that are relatively easy to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching URL Requests from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "#  requests lets you make http requests from python\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, APIs are usually special URLs that return raw data (json or XML) as opposed to a web page to be rendered for human viewers (html). Find the documentation for a particular API to see how you send requests to access whatever data you want. For example, let's try the Github API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Request data from the Github API on a particular user\n",
    "page = requests.get('https://api.github.com/users/tomdonoghue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"login\":\"TomDonoghue\",\"id\":7727566,\"avatar_url\":\"https://avatars0.githubusercontent.com/u/7727566?v=4\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/TomDonoghue\",\"html_url\":\"https://github.com/TomDonoghue\",\"followers_url\":\"https://api.github.com/users/TomDonoghue/followers\",\"following_url\":\"https://api.github.com/users/TomDonoghue/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/TomDonoghue/gists{/gist_id}\",\"starred_url\":\"https://api.github.com/users/TomDonoghue/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/TomDonoghue/subscriptions\",\"organizations_url\":\"https://api.github.com/users/TomDonoghue/orgs\",\"repos_url\":\"https://api.github.com/users/TomDonoghue/repos\",\"events_url\":\"https://api.github.com/users/TomDonoghue/events{/privacy}\",\"received_events_url\":\"https://api.github.com/users/TomDonoghue/received_events\",\"type\":\"User\",\"site_admin\":false,\"name\":\"Tom\",\"company\":\"UC San Diego\",\"blog\":\"tomdonoghue.github.io\",\"location\":\"San Diego\",\"email\":null,\"hireable\":null,\"bio\":\"Cognitive Science Grad Student @ UCSD. \\\\r\\\\nOn Twitter @TomDonoghue\",\"public_repos\":13,\"public_gists\":0,\"followers\":13,\"following\":35,\"created_at\":\"2014-05-28T20:20:48Z\",\"updated_at\":\"2018-01-09T04:15:59Z\"}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this case, the content we get back is a json file\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avatar_url             https://avatars0.githubusercontent.com/u/77275...\n",
       "bio                    Cognitive Science Grad Student @ UCSD. \\r\\nOn ...\n",
       "blog                                               tomdonoghue.github.io\n",
       "company                                                     UC San Diego\n",
       "created_at                                          2014-05-28T20:20:48Z\n",
       "email                                                               None\n",
       "events_url             https://api.github.com/users/TomDonoghue/event...\n",
       "followers                                                             13\n",
       "followers_url          https://api.github.com/users/TomDonoghue/follo...\n",
       "following                                                             35\n",
       "following_url          https://api.github.com/users/TomDonoghue/follo...\n",
       "gists_url              https://api.github.com/users/TomDonoghue/gists...\n",
       "gravatar_id                                                             \n",
       "hireable                                                            None\n",
       "html_url                                  https://github.com/TomDonoghue\n",
       "id                                                               7727566\n",
       "location                                                       San Diego\n",
       "login                                                        TomDonoghue\n",
       "name                                                                 Tom\n",
       "organizations_url          https://api.github.com/users/TomDonoghue/orgs\n",
       "public_gists                                                           0\n",
       "public_repos                                                          13\n",
       "received_events_url    https://api.github.com/users/TomDonoghue/recei...\n",
       "repos_url                 https://api.github.com/users/TomDonoghue/repos\n",
       "site_admin                                                         False\n",
       "starred_url            https://api.github.com/users/TomDonoghue/starr...\n",
       "subscriptions_url      https://api.github.com/users/TomDonoghue/subsc...\n",
       "type                                                                User\n",
       "updated_at                                          2018-01-09T04:15:59Z\n",
       "url                             https://api.github.com/users/TomDonoghue\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can read in the json data with pandas\n",
    "pd.read_json(page.content, typ='series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "This\n",
    "<a href=\"http://www.webopedia.com/TERM/A/API.html\" class=\"alert-link\">list</a>\n",
    "includes a collection of commonly used and available APIs. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Web scraping is when you (programmatically) extract data from websites.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<a href=\"https://en.wikipedia.org/wiki/Web_scraping\" class=\"alert-link\">Wikipedia</a>\n",
    "has a useful page on web scraping.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following section uses the 'BeautifulSoup' module, which is not part of the standard anaconda distribution. \n",
    "\n",
    "If you do not have BeautifulSoup, and want to get it to run this section, you can uncomment the cell below, and run it, to install BeautifulSoup in your current Python environment. You only have to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import BeautifulSoup\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the URL for the page we wish to scrape\n",
    "site_url = 'https://en.wikipedia.org/wiki/Data_science'\n",
    "\n",
    "# Launch the URL request, to get the page\n",
    "page = requests.get(site_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\"/>\\n<title>Data science - Wikipedia</title>\\n<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\\\s)client-nojs(\\\\s|$)/, \"$1client-js$2\" );</script>\\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Data_science\",\"wgTitle\":\"Data science\",\"wgCurRevisionId\":822535327,\"wgRevisionId\":822535327,\"wgArticleId\":35458904,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Use dmy dates from December 2012\",\"Information science\",\"Computer occupations\",\"Computational fields of study\",\"Data analysis\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"Ja'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the first 1000 characters of the scraped web page\n",
    "page.content[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the source of the scraped web-page is a messy pile of HTML. \n",
    "\n",
    "There is a lot of information in there, but with no clear organization. There is some structure in the page though, delineated by HTML tags, etc, we just need to use them to parse out the data. We can do that with BeautifulSoup, which takes in messy documents like this, and parses them based on a specified format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the webpage with Beautiful Soup, using a html parser\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: \n",
      "\n",
      "<title>Data science - Wikipedia</title>\n",
      "\n",
      "P-TAG:\n",
      "\n",
      "<p><b>Data science</b>, also known as <b>data-driven science</b>, is an interdisciplinary field of scientific methods, processes, and systems to extract <a href=\"/wiki/Knowledge\" title=\"Knowledge\">knowledge</a> or insights from <a href=\"/wiki/Data\" title=\"Data\">data</a> in various forms, either structured or unstructured,<sup class=\"reference\" id=\"cite_ref-:0_1-0\"><a href=\"#cite_note-:0-1\">[1]</a></sup><sup class=\"reference\" id=\"cite_ref-2\"><a href=\"#cite_note-2\">[2]</a></sup> similar to <a href=\"/wiki/Data_mining\" title=\"Data mining\">data mining</a>.</p>\n"
     ]
    }
   ],
   "source": [
    "# With the parsed soup object, we can select particular segments of the web page\n",
    "\n",
    "# Print out the page title\n",
    "print('TITLE: \\n')\n",
    "print(soup.title)\n",
    "\n",
    "# Print out the first p-tag\n",
    "print('\\nP-TAG:\\n')\n",
    "print(soup.find('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the soup object, you can explore that the page is much more organized, and in such a way that you can extract particular components of interest. \n",
    "\n",
    "Note that it is still 'messy' in other ways, in that there might or might not be a systematic structure to how the page is laid out, and it still might take a lot of work to extract the particular information you want from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs vs. Web Scraping\n",
    "\n",
    "Web scraping is distinct from using an API, even though many APIs may be accessed over the internet. Web scraping is different in that you are (programmatically) navigating through the internet, and extracting data of interest. \n",
    "\n",
    "Note:\n",
    "Be aware that scraping data from websites (without using APIs) can often be an involved project itself - scraping sites can take a considerable amount of tuning to get the data you want. \n",
    "\n",
    "Be aware that data presented on websites may not be well structured, or in an organzed format that lends itself to easy analysis.\n",
    "\n",
    "If you try scraping websites, also make sure you are allowed to scrape the data, and follow the websites terms of service. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
